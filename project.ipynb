{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "app_energy = pd.read_csv(\"/Users/achirukuri/Downloads/energydata_complete.csv\")\n",
    "app_energy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_energy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = app_energy.head(20)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.plot() # can enhance grid and see the changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the date into date time and save them in a separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_energy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(app_energy['date'])\n",
    "import dask.dataframe as dd\n",
    "app_energy['date'] = pd.to_datetime(app_energy['date'])\n",
    "app_energy.set_index(app_energy['date'],inplace=True)\n",
    "app_energy.index\n",
    "data_sample_from_pd = dd.from_pandas(app_energy, npartitions=1)\n",
    "data_sample_from_pd.index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = app_energy.resample('W').sum()\n",
    "weekly.plot(style = [':', '--', '-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = app_energy.resample('D').sum()\n",
    "daily.rolling(30, center = True).sum().plot(style =[':', '--', '-'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, model_selection, metrics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_energy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into training and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = app_energy\n",
    "# 75% of the data is usedfor the training of the models and the rest is used for testing\n",
    "train, test = train_test_split(data,test_size=0.25,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the columns based on their features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the columns based on their features like temperature, humidity anf weather (outside the house) and lights, random variables and appliances.\n",
    "\n",
    "temp = [\"T1\",\"T2\",\"T3\",\"T4\",\"T5\",\"T6\",\"T7\",\"T8\",\"T9\"]\n",
    "\n",
    "hum = [\"RH_1\",\"RH_2\",\"RH_3\",\"RH_4\",\"RH_5\",\"RH_6\",\"RH_7\",\"RH_8\",\"RH_9\"]\n",
    "\n",
    "weather = [\"T_out\", \"Tdewpoint\",\"RH_out\",\"Press_mm_hg\",\n",
    "                \"Windspeed\",\"Visibility\"] \n",
    "light = [\"lights\"]\n",
    "\n",
    "randoms = [\"rv1\", \"rv2\"]\n",
    "\n",
    "target = [\"Appliances\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating the target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vars = train[temp + hum + weather + light + randoms ]\n",
    "target_vars = train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vars.drop(columns = ['lights'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vars.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series representation of the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio.plotly as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# To understand the timeseries variation of the applaince energy consumption\n",
    "Visulaization = go.Scatter( x= data.date  ,  mode = \"lines\", y = data.Appliances )\n",
    "viz_layout = go.Layout(title = 'Appliance energy consumption of entire dataset' , xaxis=dict(title='Date'), yaxis=dict(title='(Wh)'))\n",
    "fig = go.Figure(data=[Visulaization],layout=viz_layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Weekday Variable for timeseries representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['WEEKDAY'] = ((pd.to_datetime(data['date']).dt.dayofweek)//5).astype(float)\n",
    "# There are 5472 weekend recordings \n",
    "data['WEEKDAY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series representation of all weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weekday_timeseries =  data[data['WEEKDAY'] == 0]\n",
    "# To understand the timeseries variation of the applaince energy consumption\n",
    "visData = go.Scatter( x= weekday_timeseries.date  ,  mode = \"lines\", y = weekday_timeseries.Appliances )\n",
    "layout = go.Layout(title = 'Appliance energy consumption -  weekdays' , xaxis=dict(title='Date'), yaxis=dict(title='(Wh)'))\n",
    "fig = go.Figure(data=[visData],layout=layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series representation of all Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weekend_timeseries =  data[data['WEEKDAY'] == 1]\n",
    "# To understand the timeseries variation of the applaince energy consumption\n",
    "visData = go.Scatter( x= weekend_timeseries.date  ,  mode = \"lines\", y = weekend_timeseries.Appliances )\n",
    "layout = go.Layout(title = 'Appliance energy consumption - weekends ' , xaxis=dict(title='Date'), yaxis=dict(title='(Wh)'))\n",
    "fig = go.Figure(data=[visData],layout=layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram density function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of all the features to understand the distribution\n",
    "feature_vars.hist(bins = 20 , figsize= (12,16)) ;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RH_6, RH_out, visibility, windspeed and T9 have irregular distributions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairwise plots distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(feature_vars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focussed displots for RH_6 , RH_out , Visibility , Windspeed, T9 due to irregular distribution\n",
    "f, ax = plt.subplots(2,2,figsize=(12,10))\n",
    "den1 = sns.distplot(feature_vars[\"RH_6\"],bins=10, ax= ax[0][0])\n",
    "den2 = sns.distplot(feature_vars[\"RH_out\"],bins=10, ax=ax[0][1])\n",
    "den3 = sns.distplot(feature_vars[\"Visibility\"],bins=10, ax=ax[1][0])\n",
    "den4 = sns.distplot(feature_vars[\"Windspeed\"],bins=10, ax=ax[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of values in Applainces column\n",
    "Appliances_dis = plt.figure(figsize=(12,5))\n",
    "plt.xlabel('Appliance consumption in Whatts')\n",
    "plt.ylabel('Frequency')\n",
    "sns.distplot(target_vars , bins=10 ) ;\n",
    "# postive skewed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of appliances grater than 200 wh\n",
    "data['Appliances'][data['Appliances'] > 200].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature variables have all columns and target_vars have the appliance info. \n",
    "print('Percentage of the appliance consumption is less than 200 Wh')\n",
    "print(((target_vars[target_vars <= 200].count()) / (len(target_vars)))*100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More than 90% of the appliances consume less than 200 wh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_plot = train[temp + hum + weather +target+randoms]\n",
    "corr = corr_plot.corr()\n",
    "# Mask the repeated values\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "  \n",
    "f, ax = plt.subplots(figsize=(16, 14))\n",
    "#Generate Heat Map, allow annotations and place floats in map\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\" , mask=mask,)\n",
    "    #Apply xticks\n",
    "plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "    #Apply yticks\n",
    "plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    #show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RH6 and T9 are highly correlated so remove the highly correlated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmatplot = data.corr() \n",
    "  \n",
    "f, ax = plt.subplots(figsize =(9, 8)) \n",
    "sns.heatmap(corrmatplot, ax = ax, cmap =\"YlGnBu\", linewidths = 0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the variables which are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr().unstack().sort_values(ascending = False).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_energy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average of temperatures and humidity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_energy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df = energy_dat\n",
    "df['avg_temp'] = df[['T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T_out']].mean(axis = 1)\n",
    "df['avg_RH'] = df[['RH_1', 'RH_2', 'RH_3', 'RH_4', 'RH_5', 'RH_6', 'RH_7', 'RH_8', 'RH_9', 'RH_out']].mean(axis = 1)\n",
    "df.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vars.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[feature_vars.columns]\n",
    "y_train = train[target_vars.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test[feature_vars.columns]\n",
    "y_test = test[target_vars.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Four columns have a high degree of correlation with T9 - T3,T5,T7,T8 also T6 & T_Out.\n",
    "#### rh6 most negatively correlated and T9 most positively correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.drop([\"rv1\",\"rv2\",\"Visibility\",\"RH_6\",\"T9\"],axis=1 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.drop([\"rv1\",\"rv2\",\"Visibility\",\"RH_6\", \"T9\"], axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the dataset and preprocessing using Scaling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "sc=StandardScaler()\n",
    "n = Normalizer()\n",
    "\n",
    "# Create test and training set by including Appliances column\n",
    "\n",
    "train = train[list(x_train.columns.values) + target ]\n",
    "\n",
    "test = test[list(x_test.columns.values) + target ]\n",
    "\n",
    "# Create dummy test and training set to hold scaled values\n",
    "\n",
    "sc_train = pd.DataFrame(columns=train.columns , index=train.index)\n",
    "\n",
    "sc_train[sc_train.columns] = n.fit_transform(train)\n",
    "\n",
    "sc_test= pd.DataFrame(columns=test.columns , index=test.index)\n",
    "\n",
    "sc_test[sc_test.columns] = n.fit_transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train =  sc_train.drop(['Appliances'] , axis=1)\n",
    "y_train = sc_train['Appliances']\n",
    "\n",
    "x_test =  sc_test.drop(['Appliances'] , axis=1)\n",
    "y_test = sc_test['Appliances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.to_csv('/Users/achirukuri/Desktop/test_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_csv('/Users/achirukuri/Desktop/test_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.to_csv('/Users/achirukuri/Desktop/train_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_csv('/Users/achirukuri/Desktop/test_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SKlearn to implement all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn import neighbors\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "randomforest = RandomForestRegressor(random_state = 42)\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameters currently in use:\\n')\n",
    "pprint(randomforest.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Random Search CV method for tuning the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "randomforest = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_model = RandomizedSearchCV(estimator = randomforest, param_distributions = grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = rf_model.best_params_\n",
    "#grid_accuracy = evaluate(best_grid, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "randomforest = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "randomforest.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(x_test)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For retrieving the random forest tree graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit depth of tree to 3 levels\n",
    "rf_small = RandomForestRegressor(n_estimators=10, max_depth = 7)\n",
    "rf_small.fit(x_train, y_train)\n",
    "# Extract the small tree\n",
    "tree_small = rf_small.estimators_[9] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit depth of tree to 3 levels\n",
    "rf_small = ExtraTreesRegressor(n_estimators=10, max_depth = 4)\n",
    "rf_small.fit(train_X, train_y)\n",
    "# Extract the small tree\n",
    "tree_small = rf_small.estimators_[9] \n",
    "# Save the tree as a png image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rf.fit(train_X, train_y)\n",
    "results = rf.predict(test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE  = mean_squared_error(test_X, results)  #Calculate Mean Squared Error\n",
    "RMSE = sqrt(MSE)\n",
    "print (\"RMSE : \",RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "           ['Lasso: ', Lasso()],\n",
    "           ['Ridge: ', Ridge()],\n",
    "           ['KNeighborsRegressor: ',  neighbors.KNeighborsRegressor()],\n",
    "           ['SVR:' , SVR(kernel='rbf')],\n",
    "           ['RandomForest ',RandomForestRegressor()],\n",
    "           ['ExtraTreeRegressor :',ExtraTreesRegressor()],\n",
    "           ['GradientBoostingRegressor: ', GradientBoostingRegressor()] ,\n",
    "           ['XGBRegressor: ', xgb.XGBRegressor()] ,\n",
    "           ['MLPRegressor: ', MLPRegressor(  activation='relu', solver='adam',learning_rate='adaptive',max_iter=1000,learning_rate_init=0.01,alpha=0.01)]\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model_data = []\n",
    "for name,curr_model in models :\n",
    "    curr_model_data = {}\n",
    "    curr_model.random_state = 78\n",
    "    curr_model_data[\"Name\"] = name\n",
    "    start = time.time()\n",
    "    curr_model.fit(train_X,train_y)\n",
    "    end = time.time()\n",
    "    curr_model_data[\"Train_Time\"] = end - start\n",
    "    curr_model_data[\"Train_R2_Score\"] = metrics.r2_score(train_y,curr_model.predict(train_X))\n",
    "    curr_model_data[\"Test_R2_Score\"] = metrics.r2_score(test_y,curr_model.predict(test_X))\n",
    "    curr_model_data[\"Test_RMSE_Score\"] = sqrt(mean_squared_error(test_y,curr_model.predict(test_X)))\n",
    "    curr_model_data[\"Train_RMSE_Score\"] = sqrt(mean_squared_error(train_y,curr_model.predict(train_X)))\n",
    "    model_data.append(curr_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1:,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created a dataframe and the data points are the values which are obtained from the models in the R code except extrta regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['MARS', 0.075, 0.995], ['Ridge', 0.0334, 0.909], ['Lasso', 0.0334, 0.909], ['GBM', 0.004, 0.998], ['SVM', 0.0096,0.993], \n",
    "       ['KNN',0.02470946, 0.95065926], ['Decision Trees', 0.03289199, 0.90866495], ['PLS', 0.03372,0.9079], ['Enet', 0.0354,0.9038],\n",
    "       ['Extratree Regressor', 0.004421, 0.998350], ['Random Forest', 0.0063, 0.9968]]\n",
    "df = pd.DataFrame(data, columns = ['Name', 'RMSE', 'R2']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by = ['R2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=\"Name\", y=['R2' , 'RMSE'], kind=\"bar\" ,\n",
    "        title = 'RMSE, R2 Score Results' , figsize= (6,6)) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [{\n",
    "              'max_depth': [80, 150, 200,250],\n",
    "              'n_estimators' : [100,150,200,250],\n",
    "              'max_features': [\"auto\", \"sqrt\", \"log2\"]\n",
    "            }]\n",
    "reg = ExtraTreesRegressor(random_state=40)\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = reg, param_grid = param_grid, cv = 5, n_jobs = -1 , scoring='r2' , verbose=2)\n",
    "grid_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.score(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.score(test_X,test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(test_y, grid_search.best_estimator_.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning xtree regressor : \n",
    "    R2: 0.998901\n",
    "    RMSE: 0.003607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(test_y,grid_search.best_estimator_.predict(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indices = np.argsort(grid_search.best_estimator_.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = grid_search.best_estimator_.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "names = [train_X.columns[i] for i in indices]\n",
    "# Create plot\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Create plot title\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "# Add bars\n",
    "plt.bar(range(train_X.shape[1]), importances[indices])\n",
    "\n",
    "# Add feature names as x-axis labels\n",
    "plt.xticks(range(train_X.shape[1]), names, rotation=90)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_important_feature = train_X[names[0:5]]\n",
    "test_important_feature = test_X[names[0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "cloned_model = clone(grid_search.best_estimator_)\n",
    "cloned_model.fit(train_important_feature , train_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
